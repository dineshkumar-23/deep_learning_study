{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lstm_text_generation",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.7 64-bit",
      "metadata": {
        "interpreter": {
          "hash": "619d769253c6373b04549ec5b49989d4393576b7f88924d884f5804d533fbfa4"
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8oR_A0NQ2IA"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Embedding, LSTM, Bidirectional, Dropout\n",
        "\n",
        "import email\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.translate.bleu_score import sentence_bleu"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTLQWaYTwuvg",
        "outputId": "f41e640b-a2d2-41e3-9355-f15849cb5574"
      },
      "source": [
        "#Read dataset\r\n",
        "file = pd.read_csv('emails.csv', nrows=1000)\r\n",
        "print(file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0A8xirawxig"
      },
      "source": [
        "#Helper function for extracting email body from raw email\r\n",
        "def get_text_from_email(msg):\r\n",
        "        parts = []\r\n",
        "        for part in msg.walk():\r\n",
        "            if part.get_content_type() == 'text/plain':\r\n",
        "                parts.append( part.get_payload() )\r\n",
        "        return ''.join(parts)\r\n",
        "\r\n",
        "#Helper function for seggregating the fields present in emails\r\n",
        "def split_email_addresses(line):\r\n",
        "    if line:\r\n",
        "        addrs = line.split(',')\r\n",
        "        addrs = list(frozenset(map(lambda x: x.strip(), addrs)))\r\n",
        "    else:\r\n",
        "        addrs = None\r\n",
        "    return addrs\r\n",
        "\r\n",
        "def preprocessing_emails(dataframe):\r\n",
        "        email_df = dataframe\r\n",
        "        messages = list(map(email.message_from_string,email_df['message']))\r\n",
        "        keys = messages[0].keys()\r\n",
        "        for key in keys:\r\n",
        "            email_df[key] = [doc[key] for doc in messages]\r\n",
        "        email_df['email_body'] = list(map(get_text_from_email, messages))\r\n",
        "        email_df.drop(['file', 'message', 'Message-ID', 'Date', 'From', 'To', 'Subject', 'Mime-Version', 'Content-Type', 'Content-Transfer-Encoding', 'X-From', 'X-To', 'X-cc', 'X-bcc', 'X-Folder', 'X-Origin', 'X-FileName'], axis=1, inplace=True)\r\n",
        "        return email_df\r\n",
        "\r\n",
        "df = preprocessing_emails(file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4j_20yo0wzZP"
      },
      "source": [
        "def tokenize_corpus(corpus, num_words=-1):\r\n",
        "  # Fit a Tokenizer on the corpus\r\n",
        "  if num_words > -1:\r\n",
        "    tokenizer = Tokenizer(num_words=num_words)\r\n",
        "  else:\r\n",
        "    tokenizer = Tokenizer()\r\n",
        "  tokenizer.fit_on_texts(corpus)\r\n",
        "  return tokenizer\r\n",
        "\r\n",
        "def create_corpus(dataset, field):\r\n",
        "  # Make it lowercase\r\n",
        "  dataset[field] = dataset[field].str.lower()\r\n",
        "  # Make it one long string to split by line\r\n",
        "  message = dataset[field].str.cat()\r\n",
        "  corpus = message.split('.')\r\n",
        "  return corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQoKU-4Fw24I",
        "outputId": "c55edcfb-b7e7-4aa5-9697-90ac8c6ca19b"
      },
      "source": [
        "corpus = create_corpus(df, 'email_body')\r\n",
        "#print(len(corpus))\r\n",
        "tokens = tokenize_corpus(corpus)\r\n",
        "total_words = len(tokens.word_index) + 1\r\n",
        "#print(total_words)\r\n",
        "print(corpus[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCpZD5T6w4eh"
      },
      "source": [
        "def get_sequences(corpus):\r\n",
        "    sequences = []\r\n",
        "    for line in corpus:\r\n",
        "        token_list = tokens.texts_to_sequences([line])[0]\r\n",
        "        for i in range(1, len(token_list)):\r\n",
        "            n_gram_sequence = token_list[:i+1]\r\n",
        "            sequences.append(n_gram_sequence)\r\n",
        "    return sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZIfUY3cw5eT"
      },
      "source": [
        "sequences = get_sequences(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSuxH4gzw93E",
        "outputId": "f29dbb84-6c53-4271-8954-a099f4a06b3a"
      },
      "source": [
        "max_sequence_len = max([len(seq) for seq in sequences])\r\n",
        "padded_sequences = np.array(pad_sequences(sequences, maxlen=max_sequence_len, padding='post'))\r\n",
        "#splitting input and output variables\r\n",
        "input_sequences, labels = padded_sequences[:,:-1], padded_sequences[:,-1]\r\n",
        "#one hot encoding labels\r\n",
        "labels = tf.keras.utils.to_categorical(labels, num_classes=total_words)\r\n",
        "input_sequences.shape, labels.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1_VUVJbxBdB",
        "outputId": "ee216759-00c6-4778-af25-81d584cc9fc3"
      },
      "source": [
        "model = Sequential()\r\n",
        "model.add(Embedding(total_words, 64, input_length=max_sequence_len-1))\r\n",
        "model.add(Bidirectional(LSTM(100)))\r\n",
        "model.add(Dropout(0.2))\r\n",
        "model.add(Dense(total_words, activation='softmax'))\r\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\r\n",
        "history = model.fit(input_sequences, labels, epochs=100, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dP_I5rVwvrkC"
      },
      "source": [
        "import os.path\r\n",
        "if os.path.isfile('models/text_gen_model_2.h5') is False:\r\n",
        "  model.save('models/text_gen_model_2.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2Bq7xH5wWDZ",
        "outputId": "f84e86ca-929e-4478-82b8-eda4aafa116f"
      },
      "source": [
        "seed_text = 'suggest holding the business.'\r\n",
        "next_words = 50\r\n",
        "\r\n",
        "for _ in range(next_words):\r\n",
        "  token_list = tokens.texts_to_sequences([seed_text])[0]\r\n",
        "  token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='post')\r\n",
        "  predicted = np.argmax(model.predict(token_list), axis=-1)\r\n",
        "  output_word = \"\"\r\n",
        "  for word, index in tokens.word_index.items():\r\n",
        "    if index == predicted:\r\n",
        "      output_word = word\r\n",
        "      break\r\n",
        "  seed_text += \" \" + output_word\r\n",
        "print(seed_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pg7TlDXbyY_3",
        "outputId": "bff4c096-1511-4ca6-c337-1b6bd8585b87"
      },
      "source": [
        "from tensorflow.keras.models import load_model\r\n",
        "new_model = load_model('models/text_gen_model_2.h5')\r\n",
        "# Use this process for the full output generation\r\n",
        "seed_text = \"suggest holding the business.\"\r\n",
        "next_words = 100\r\n",
        "\r\n",
        "for _ in range(next_words):\r\n",
        "  token_list = tokens.texts_to_sequences([seed_text])[0]\r\n",
        "  #print(token_list)\r\n",
        "  token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\r\n",
        "  #print(token_list)\r\n",
        "  predicted_probs = new_model.predict(token_list)[0]\r\n",
        "  predicted = np.random.choice([x for x in range(len(predicted_probs))],\r\n",
        "                               p=predicted_probs)\r\n",
        "  output_word = \"\"\r\n",
        "  for word, index in tokens.word_index.items():\r\n",
        "    if index == predicted:\r\n",
        "      output_word = word\r\n",
        "      break\r\n",
        "  seed_text += \" \" + output_word\r\n",
        "print(seed_text)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}